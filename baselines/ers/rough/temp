# class RAT:
#     def __init__(self, envs: List[gym.Env], eval_env: gym.Env, seed=0, gamma=0.99, experience_buffer_length=10000, exploration_period=1000, dup_q_update_interval=500, update_interval=4, minibatch_size=32, pretrain_trainer=True, pretraining_steps=100, timesteps=1e7, ob_dtype='float32', learning_rate=1e-3, render=False):        
#         config = tf.ConfigProto(device_count = {'GPU': 0})
#         sess = tf.Session(config=config)

#         self.ob_shape = env.observation_space.shape
#         self.ac_shape = env.action_space.shape
#         self.ob_dtype = ob_dtype
#         self.learning_rate = learning_rate
#         self.n_recommenders = len(envs)
#         self.experience_buffer_length = experience_buffer_length
#         self.exploration_period = exploration_period
#         self.update_interval = update_interval
#         self.minibatch_size = minibatch_size
#         self.pretrain_trainer = pretrain_trainer
#         self.pretraining_steps = pretraining_steps
#         self.gamma = gamma
#         self.timesteps = timesteps
#         self.seed = seed
#         self.render = render
#         self.env = env
#         self.eval_env = eval_env
#         self.dup_q_update_interval = dup_q_update_interval
#         self.experiences = [] # type: List[Experience]
#         self.thinking_steps_before_acting = 0
#         self.trainer_training_steps = 1
#         self.recommenders_training_steps = 1
#         self.use_beam_search = True
#         self.beam_selection_interval = 1
#         self.epsilon_final = 0.1
#         self.epsilon_anneal = 2000
#         self.update_count = 0
#         self.evaluation_envs_count = 4

#         self.recommenders = [Recommender(sess, 'r{0}'.format(i), self.ob_shape, self.ac_shape, self.ob_dtype) for i in range(n_recommenders)]
#         self.actor_trainer = ActorTrainer(sess, self.recommenders, self.ob_shape, self.ac_shape, self.ob_dtype, self.learning_rate)
#         sess.run(tf.global_variables_initializer())

#         # for variation... let each recommender be mutation of another:
#         for r, r_mut in zip(self.recommenders[:-1], self.recommenders[1:]):
#             r.copy_to(r_mut, mutation_probability=0.5)
        

#     def _get_action(self, state, best_action=True, epsilon=0.1):
#         a, q = self._get_actions([state], best_actions=best_action, epsilon=epsilon)
#         return a[0], q[0]

#     def _get_actions(self, states, best_actions=True, epsilon=0.1):
#         recos, qs = self.actor_trainer.get_recommendations(states)
#         actions = []
#         q = []
#         for i in range(len(states)):
#             best_action_index = np.argmax(qs[:,i])
#             best_action = recos[best_action_index, i]
#             best_q = qs[best_action_index, i]
#             if best_actions:
#                 actions.append(best_action)
#                 q.append(best_q)
#             else:
#                 #print('epsilon: {0}'.format(epsilon))
#                 rand = np.random.rand()
#                 if rand < 1 - epsilon:
#                     actions.append(best_action)
#                     q.append(best_q)
#                 elif rand < 1 - epsilon*epsilon:
#                     random_action_index = np.random.randint(len(self.recommenders))
#                     action = recos[random_action_index, i]
#                     noisy_action = np.clip(action + np.random.standard_normal(size=np.shape(action))/10, 0, 1)
#                     actions.append(noisy_action)
#                     q.append(qs[random_action_index, i])
#                     print('Noisy action')
#                 elif rand <= 1:
#                     actions.append(self.env.action_space.sample())
#                     q.append(0)
#                     print('Random action')
#                 else:
#                     raise RuntimeError('This should not happen. rand is {0}'.format(rand))
#                 for j in range(len(self.recommenders)):
#                     self.recommenders[j].add_score(qs[j, i])
            
#         return actions, q
    
#     def _get_epsilon(self):
#         if self.t < self.exploration_period:
#             return 1
#         if self.t >= (self.exploration_period + self.epsilon_anneal):
#             return self.epsilon_final
#         return 1 - (1 - self.epsilon_final) * (self.t - self.exploration_period)/(self.epsilon_anneal)



#     def _should_train_trainer(self):
#         ans = self.t > self.exploration_period and self.t % self.update_interval == 0
#         return ans

#     def _should_train_recommenders(self):
#         ans = self.t > self.exploration_period and self.t % self.update_interval == 0
#         return ans

#     def _should_select_recommenders(self):
#         ans = self.use_beam_search and self.episodes % self.beam_selection_interval == 0
#         if ans and self.n_recommenders % 2 != 0:
#             raise RuntimeError('To do selection, number of recommenders should be even')
#         return ans

#     def _should_update_duplicate_q(self):
#         return self.update_count % self.dup_q_update_interval == 0

#     def _think_before_acting(self, state):
#         for step in range(self.thinking_steps_before_acting):
#             self.actor_trainer.train_recommenders([state])

#     def _train_trainer(self, steps):
#         for step in range(steps):
#             exps = list(self._random_experiences(self.minibatch_size)) # type: List[Experience]
#             next_states = [e.next_state for e in exps]
#             # pass best_actions=False to do on-policy learning
#             next_acts, next_qs = self._get_actions(next_states, best_actions=True)
#             target_qs = [0] * self.minibatch_size
#             for i in range(self.minibatch_size):
#                 target_qs[i] = exps[i].reward
#                 if not exps[i].done:
#                     target_qs[i] += self.gamma * next_qs[i]
#             loss, _ = self.actor_trainer.train_q([e.state for e in exps], [e.action for e in exps], target_qs)
#             #print('loss: {0}'.format(loss))
#             self.update_count += 1
#             if self._should_update_duplicate_q():
#                 self.actor_trainer.update_duplicate_q()

#     def _train_recommenders(self, steps):
#         for step in range(steps):
#             exps = self._random_experiences(self.minibatch_size)
#             states = [e.state for e in exps]
#             av_q, _ = self.actor_trainer.train_recommenders(states)
#             #print('av_q: {0}'.format(av_q))

#     def _eval_recommenders(self):
#         base_seed = np.random.randint(0, 100)
#         scores = []
#         print('Evaluating recommenders on base_seed {0}'.format(base_seed))
#         for reco in self.recommenders:
#             #print('Reco #{0} :'.format(len(scores)))
#             R = []
#             for test_no in range(self.evaluation_envs_count):
#                 seed = base_seed + test_no
#                 self.eval_env.seed(seed)
#                 obs = self.eval_env.reset()
#                 ep_r = 0
#                 d = False
#                 while not d:
#                     a = reco.get_recommendations([obs])[0]
#                     obs, r, d, _ = self.eval_env.step(a)
#                     ep_r += r
#                 #print(ep_r)
#                 R.append(ep_r)
#             scores.append(np.average(R))
#         print('Evalution scores: {0}'.format(scores))
#         return np.array(scores)
#         #return np.array([reco.average_score() for reco in self.recommenders])
            

#     def _select_recommenders(self):
#         p = self._eval_recommenders()
#         p -= np.min(p)
#         if np.max(p) > 0:
#             p /= (np.max(p)/4)
#         else:
#             p = np.array([1]*len(p))
#         p = np.exp(p)
#         p /= np.sum(p)
#         selected = np.random.choice(len(p), p=p, size=len(p)//2, replace=False)
#         notSelected = list(filter(lambda i: i not in selected, range(len(p))))
#         mutation_probs = np.linspace(0.1, 0.5, num=len(selected))
#         #print('Selected recommenders:')
#         #print(selected)
#         #print('To be replaced recommenders:')
#         #print(list(notSelected))
#         #print('Mutation probs:')
#         #print(mutation_probs)
#         for s, ns, mp in zip(selected, notSelected, mutation_probs):
#             #print(mp)
#             self.recommenders[s].copy_to(self.recommenders[ns], mutation_probability=mp)
#             self.recommenders[ns].reset_age()
#         # for r in self.recommenders:
#         #     r.reset_scores()

#     def learn(self):
#         #for i in range(100):
#         #    self.actor_trainer.train_q([np.random.rand(self.ob_shape[0]), np.random.rand(self.ob_shape[0])], [[-2],[2]], [-10,-10])

#         self.env.seed(self.seed)
#         self.t = 0
#         self.episodes = 0
#         self.rewards = []
#         for r in self.recommenders:
#             r.reset_scores()
#         while self.t < self.timesteps:
#             if self._should_select_recommenders():
#                 self._select_recommenders()
#             for r in self.recommenders:
#                 r.reset_scores()
#             obs = self.env.reset()
#             if self.render: self.env.render()
#             R = 0
#             d = False
#             while not d:
#                 # general training
#                 if self.pretrain_trainer and self.t == self.exploration_period:
#                     print('Exploration phase ended. Pretraining trainer:')
#                     self._train_trainer(self.pretraining_steps)
#                 if self._should_train_trainer():
#                     self._train_trainer(self.trainer_training_steps)
#                 if self._should_train_recommenders():
#                     self._train_recommenders(self.recommenders_training_steps)
                
#                 self._think_before_acting(obs)
#                 a, q = self._get_action(obs, best_action=False, epsilon=self._get_epsilon())
#                 next_obs, r, d, info = self.env.step(a)
#                 self._add_to_experience(Experience(obs, a, r, d, info, next_obs))
#                 obs = next_obs
#                 R += r
#                 if self.render: self.env.render()
#                 self.t += 1
            
#             for r in self.recommenders:
#                 r.increment_age()
#             print('Scores of recommenders:')
#             p = np.array([r.average_score() for r in self.recommenders])
#             print(p)
            
#             self.episodes += 1
#             self.rewards.append(R)
#             print('episode {0}:\t{1}'.format(self.episodes,R))
#             if sum(self.rewards[-5:]) > (400*5):
#                 break



        # self.calculate_epsilons(self.ac_space.high)

    def calculate_epsilons(self, constraints: np.ndarray, inputs_shape):
        '''
        for some epsilons vector,
        our output z needs to be (exp + epsilons)/(sigma + sum(epsilons))
        to satisfy the constraints, we get the following set of linear equations:
        for all i:
            (constraints[i] - 1) * epsilons[i] + constraints[i] * sum(epsilons[except i]) = 1 - constraints[i]
        '''
        if np.any(constraints < 0) or np.any(constraints > 1):
            raise ValueError(
                "constraints needs to be in range [0, 1]")
        if np.sum(constraints) <= 1:
            raise ValueError("sum of constraints need to be greater than 1")

        dimensions = reduce(mul, inputs_shape, 1)
        constraints = np.asarray(constraints)
        constraints_flat = constraints.flatten()
        # to solve the epsilons linear equation:
        # coefficient matrix:
        coeffs = np.array([[(constraints_flat[row] - 1 if col == row else constraints_flat[row])
                            for col in range(dimensions)] for row in range(dimensions)])
        constants = np.array([1 - constraints_flat[row]
                              for row in range(dimensions)])
        epsilons_flat = np.linalg.solve(coeffs, constants)
        epsilons = np.reshape(epsilons_flat, inputs_shape)
        logger.log("wrapper: episilons are {0}".format(
            epsilons), level=logger.INFO)
        return epsilons

    def softmax_max(self, inputs: np.ndarray, max_constraints, epsilons=None):
        """assumes that inputs lie between range 0 and 1"""
        # y = inputs - tf.reduce_max(inputs, axis=1, keepdims=True)
        # y = tf.minimum(inputs, 0)
        # exp = tf.exp(y)
        sigma = np.sum(inputs, axis=-1, keepdims=True)
        if epsilons is None:
            epsilons = self.calculate_epsilons(
                max_constraints, inputs_shape=inputs.shape)
        epsilons_sigma = np.sum(epsilons)
        return (inputs + epsilons) / (sigma + epsilons_sigma)

    def _nested_softmax(self, inputs, constrained_node, z_tree={}, z_node=None):
        if constrained_node["equals"] is not None:
            assert len(z_tree.keys(
            )) == 0, "An equals_constraint was encountered at a non-root node! Node name: {0}".format(constrained_node["name"])
            assert constrained_node["equals"] == constrained_node["min"] == constrained_node[
                "max"], "At root node, min, max & equal constraints should be same"
            # initialize z_tree:
            z_tree["tensor"] = constrained_node["equals"]
            z_tree["equals"] = constrained_node["equals"]
            z_tree["min"] = constrained_node["min"]
            z_tree["max"] = constrained_node["max"]
            z_tree["name"] = constrained_node["name"]
            z_tree["consumed_inputs"] = 0
            z_node = z_tree

        # base_case: if this the leaf node, return the tensor
        if "children" not in constrained_node or len(constrained_node["children"]) == 0:
            z_node["zone_id"] = constrained_node["zone_id"]
            return [z_node]

        '''
        M = max_constraints of children\n
        m = min_constraints of children\n
        We will distribute min_constraints to the respective nodes first. \n
        The remaining sum will be: \n
            s = z_node["tensor"] - sum(m) \n
        The maximum value of this sum can be:
            S = z_node["max"] - sum(m) \n
        Then we find vector u s.t. sum(u)=s. and u_i in (0, M_i-m_i) \n
            u = s * max_constrained_softmax(inputs, (M-m)/S) \n
        Then: \n
            z_children_i = m_i + u_i \n
        i.e. \n
            z_children_i = m_i + s * max_constrained_softmax(inputs, (M - m)/S)_i \n

        Please convince yourself that z_children_i will be in range (m_i, M_i) and sum(z_children) = z_node["tensor"]
        '''

        children = constrained_node["children"]
        max_constraints = np.array([c["max"] for c in children])
        min_constraints = np.array([c["min"] for c in children])
        assert 0 <= np.sum(min_constraints) <= z_node["min"] <= z_node["max"] < np.sum(
            max_constraints), "The constraints should satisfy 0 <= sum(children_min) <= parent_min <= parent_max < sum(children_max)"
        assert np.all(0 <= min_constraints) and np.all(min_constraints < max_constraints) and np.all(
            max_constraints <= 1), "The constraints should satisfy 0 <= min_constraint_i < max_constraint_i <= 1"

        # # print(constrained_node['name'])
        # if constrained_node['equals'] is not None:
        #     # then this is root node. we need not compute epsilons dynamically
        #     s = z_node["tensor"] - np.sum(min_constraints)
        #     u_max_constraints = (max_constraints - min_constraints) / s
        #     u_max_constraints = np.minimum(u_max_constraints, 1)
        #     u = s * tf_softmax_with_max_constraints(inputs[:, z_tree["consumed_inputs"]:z_tree["consumed_inputs"] + len(
        #         children)], u_max_constraints, scope="CSMM_on_children_of_{0}".format(z_node["name"]))
        #     z_tree["consumed_inputs"] += len(children)
        #     z_children_tensors = min_constraints + u
        # else:
        s = z_node["tensor"] - np.sum(min_constraints)
        u_max_constraints = (max_constraints - min_constraints) / s
        u_max_constraints = np.minimum(u_max_constraints, 1.0)
        u = s * self.softmax_max(inputs[z_tree["consumed_inputs"]:z_tree["consumed_inputs"] + len(children)], u_max_constraints)
        z_tree["consumed_inputs"] += len(children)
        z_children_tensors = min_constraints + u

        z_node["children"] = []
        for i in range(len(children)):
            child = {}
            z_node["children"].append(child)
            child["tensor"] = z_children_tensors[i:i + 1]
            child["name"] = children[i]["name"]
            child["min"] = children[i]["min"]
            child["max"] = children[i]["max"]
            child["equals"] = children[i]["equals"]

        return_nodes = []
        for z_node_child, constrained_node_child in zip(z_node["children"], constrained_node["children"]):
            c_returned_nodes = self._nested_softmax(
                inputs, constrained_node_child, z_tree=z_tree, z_node=z_node_child)
            return_nodes.extend(c_returned_nodes)

        return return_nodes

    def nested_softmax(self, inputs, constraints, z_tree={}):
        z_tree = {}
        leaf_z_nodes = self._nested_softmax(inputs, constraints, z_tree=z_tree)
        leaf_z_tensors = [leaf['tensor'] for leaf in sorted(
            leaf_z_nodes, key=lambda leaf:leaf["zone_id"])]
        return np.concatenate(leaf_z_tensors, axis=-1)